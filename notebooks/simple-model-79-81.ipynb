{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d195c6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.020196,
     "end_time": "2023-01-14T21:49:24.732280",
     "exception": false,
     "start_time": "2023-01-14T21:49:24.712084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfab76e",
   "metadata": {
    "papermill": {
     "duration": 30.965122,
     "end_time": "2023-01-14T21:49:55.703188",
     "exception": false,
     "start_time": "2023-01-14T21:49:24.738066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/hi-paris-2023/train/train_features_sent.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/hi-paris-2023/test/test_features_sent.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1953d90",
   "metadata": {
    "papermill": {
     "duration": 0.005588,
     "end_time": "2023-01-14T21:49:55.715397",
     "exception": false,
     "start_time": "2023-01-14T21:49:55.709809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d6d3d",
   "metadata": {
    "papermill": {
     "duration": 135.265794,
     "end_time": "2023-01-14T21:52:10.987204",
     "exception": false,
     "start_time": "2023-01-14T21:49:55.721410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_lowe_floor_thermal_conductivity(df):\n",
    "    df['lowe_floor_thermal_conductivity'] = df[['lowe_floor_thermal_conductivity']].fillna(2.8)\n",
    "    return df\n",
    "\n",
    "def process_lower_floor_insulation_type(df):\n",
    "    np.random.seed(0)\n",
    "    insulation_df = pd.DataFrame()\n",
    "    insulation_df['external_insulation'] = df['lower_floor_insulation_type'].str.contains('external').fillna(False).astype(int)\n",
    "    insulation_df['internal_insulation'] = df['lower_floor_insulation_type'].str.contains('internal').fillna(False).astype(int)\n",
    "    \n",
    "    undetermined_insulation_mask = df['lower_floor_insulation_type'] == 'insulated'\n",
    "    insulation_df[undetermined_insulation_mask] = np.random.randint(low=0, high=2, size=(np.sum(undetermined_insulation_mask), 2))\n",
    "    \n",
    "    df = pd.concat([df.drop(columns=['lower_floor_insulation_type']), insulation_df], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_lower_floor_material(df):\n",
    "    floor_df = pd.DataFrame()\n",
    "    floor_df['concrete_slab_floor'] = (df['lower_floor_material'] == 'concrete slab')\n",
    "    floor_df['heavy_floor'] = (df['lower_floor_material'] == 'heavy floor, such as clay floor joists, concrete beams')\n",
    "    floor_df['insulated_joist_floor'] = (df['lower_floor_material'] == 'Insulated joist floor')\n",
    "    floor_df['wood_floor'] = df['lower_floor_material'].str.contains('wood', case=False)\n",
    "    floor_df['metal_floor'] = df['lower_floor_material'].str.contains('metal', case=False)\n",
    "    \n",
    "    floor_df['other_floor'] = ~floor_df.any(axis=1)\n",
    "    \n",
    "    floor_df = floor_df.fillna(False).astype(int)\n",
    "    \n",
    "    df = pd.concat([df.drop(columns=['lower_floor_material']), floor_df], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_lower_floor_adjacency_type(df):\n",
    "    np.random.seed(0)\n",
    "    prob_series = df['lower_floor_adjacency_type'].value_counts(normalize=True)\n",
    "    categories = prob_series.index.tolist()\n",
    "    probabilities = prob_series.values.tolist()\n",
    "   \n",
    "    col_na_mask = df['lower_floor_adjacency_type'].isna()\n",
    "    df['lower_floor_adjacency_type'][col_na_mask] = np.random.choice(categories, size=np.sum(col_na_mask), p=probabilities)\n",
    "   \n",
    "    one_hot_df = pd.get_dummies(df['lower_floor_adjacency_type'], prefix='lower_floor_adjacency_type', drop_first=True)\n",
    "   \n",
    "    df = pd.concat([df.drop(columns='lower_floor_adjacency_type'), one_hot_df], axis=1)\n",
    "   \n",
    "    return df\n",
    "\n",
    "def process_cat_pct(df, column, new_col_name=None):\n",
    "    cat_data_pct = df[column].value_counts().cumsum()/len(df)\n",
    "    keep_cats = cat_data_pct[cat_data_pct<0.95].index.tolist()\n",
    "    \n",
    "    if cat_data_pct.iloc[0] > 0.3:\n",
    "        df[column] = df[column].fillna(cat_data_pct.index[0])\n",
    "        \n",
    "    if not new_col_name:\n",
    "        new_col_name = column\n",
    "    \n",
    "    for category in keep_cats:\n",
    "        df[f'{new_col_name}_{category}'] = (df[column] == category).astype(int)\n",
    "        \n",
    "    df.drop(columns=[column], inplace = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_one_hot_ColumnSplitBySymbol(df, col_name, symbol): #symbol = ' + '\n",
    "    df[col_name] = df[col_name].fillna('none')\n",
    "    df[col_name] = df[col_name].apply(lambda x: 'none' if x=='' else x)\n",
    "    new_df = pd.get_dummies(pd.DataFrame(df[col_name].apply(lambda x: x.split(symbol)).tolist()))\n",
    "    new_df.columns = new_df.columns.str.split(\"_\").str[-1]\n",
    "\n",
    "    merged_df = pd.DataFrame()\n",
    "    for unique_column in new_df.columns.unique():\n",
    "        if len(new_df[unique_column].shape) == 1:\n",
    "            merged_df[f'{col_name}_{unique_column}_merged'] = new_df[unique_column]\n",
    "        else:\n",
    "            merged_df[f'{col_name}_{unique_column}_merged'] = np.zeros(new_df[unique_column].shape[0], dtype=np.int32)\n",
    "            for icol in range(new_df[unique_column].shape[1]):\n",
    "                merged_df[f'{col_name}_{unique_column}_merged'] = merged_df[f'{col_name}_{unique_column}_merged'] | new_df[unique_column].iloc[:,icol]\n",
    "    df = pd.concat([df, merged_df], axis=1)\n",
    "    df.drop(columns=[col_name], inplace=True)\n",
    "    return df\n",
    "\n",
    "def create_one_hot_ColumnOfLists(df, col_name):\n",
    "    df[col_name] = df[col_name].fillna(df[col_name].mode().iloc[0])\n",
    "    df[col_name] = df[col_name].apply(lambda x: '[empty]' if x=='[]' else x)\n",
    "    new_df = pd.get_dummies(pd.DataFrame(df[col_name].apply(lambda x: x[1:-1].split(',')).tolist()))\n",
    "    new_df.columns = new_df.columns.str.split(\"_\").str[-1]\n",
    "\n",
    "    merged_df = pd.DataFrame()\n",
    "    for unique_column in new_df.columns.unique():\n",
    "        if len(new_df[unique_column].shape) == 1:\n",
    "            merged_df[f'{col_name}_{unique_column}_merged'] = new_df[unique_column]\n",
    "        else:\n",
    "            merged_df[f'{col_name}_{unique_column}_merged'] = np.zeros(new_df[unique_column].shape[0], dtype=np.int32)\n",
    "            for icol in range(new_df[unique_column].shape[1]):\n",
    "                merged_df[f'{col_name}_{unique_column}_merged'] = merged_df[f'{col_name}_{unique_column}_merged'] | new_df[unique_column].iloc[:,icol]\n",
    "    df = pd.concat([df, merged_df], axis=1)\n",
    "    df.drop(columns=[col_name], inplace=True)\n",
    "    return df\n",
    "\n",
    "def create_one_hot(df, col_name):\n",
    "    df[col_name] = df[col_name].fillna(df[col_name].mode().iloc[0])\n",
    "    df = pd.concat([df, pd.get_dummies(df[col_name])], axis=1)\n",
    "    df.drop(columns=[col_name], inplace=True)\n",
    "    return df\n",
    "\n",
    "def clean_1to24(data):\n",
    "    data2 = data.copy()\n",
    "    \n",
    "    to_one_hot=[]\n",
    "    data2 = re_categorize_by_count(data2,\"additional_heat_generators\", 1000)\n",
    "    data2 = re_categorize_by_count(data2,\"additional_water_heaters\", 1000)\n",
    "\n",
    "    data2[\"altitude\"] = data2[\"altitude\"].fillna(data2[\"altitude\"].mean())\n",
    "    \n",
    "    data2.drop(columns=[\"balcony_depth\"], inplace=True)\n",
    "    \n",
    "    data2 =create_one_hot_ColumnSplitBySymbol(data2, \"bearing_wall_material\", \" - \")\n",
    "    data2 = create_one_hot_ColumnOfLists(data2,\"building_category\")\n",
    "\n",
    "    data2 = create_one_hot_ColumnOfLists(data2,\"building_class\")\n",
    "    data2[\"building_height_ft\"] = data2[\"building_height_ft\"].fillna(data2[\"building_height_ft\"].median())      \n",
    "\n",
    "    data2[\"building_total_area_sqft\"] = data2[\"building_total_area_sqft\"].fillna(data2[\"building_total_area_sqft\"].median())\n",
    "    data2[\"living_area_sqft\"] = data2[\"living_area_sqft\"].fillna(data2[\"living_area_sqft\"].median())\n",
    "    \n",
    "    to_one_hot.append(\"building_type\")\n",
    "    to_one_hot.append(\"building_use_type_code\")\n",
    "        \n",
    "    for col in to_one_hot:\n",
    "        data2 = create_one_hot(data2, col)\n",
    "    \n",
    "    return data2\n",
    "\n",
    "def re_categorize_by_count(df, col_name, threshold):\n",
    "    df[col_name] = df[col_name].fillna(df[col_name].mode().iloc[0])\n",
    "    df[col_name] = df[col_name].apply(lambda x: '[empty]' if x=='[]' else x)\n",
    "    \n",
    "    dict_col = df[col_name].value_counts().to_dict()\n",
    "    df[col_name] = df[col_name].apply(lambda x: '[other]' if dict_col[x]<threshold else x)\n",
    "    \n",
    "    new_df = pd.get_dummies(pd.DataFrame(df[col_name].apply(lambda x: x[1:-1].split(',')).tolist()))\n",
    "    new_df.columns = new_df.columns.str.split(\"_\").str[-1]\n",
    "\n",
    "    merged_df = pd.DataFrame()\n",
    "    for unique_column in new_df.columns.unique():\n",
    "        if len(new_df[unique_column].shape) == 1:\n",
    "            merged_df[f'{col_name}_{unique_column}_merged'] = new_df[unique_column]\n",
    "        else:\n",
    "            merged_df[f'{col_name}_{unique_column}_merged'] = np.zeros(new_df[unique_column].shape[0], dtype=np.int32)\n",
    "            for icol in range(new_df[unique_column].shape[1]):\n",
    "                merged_df[f'{col_name}_{unique_column}_merged'] = merged_df[f'{col_name}_{unique_column}_merged'] | new_df[unique_column].iloc[:,icol]\n",
    "    df = pd.concat([df, merged_df], axis=1)\n",
    "    df.drop(columns=[col_name], inplace=True)\n",
    "    return df\n",
    "    \n",
    "def clean_features(orig_df):\n",
    "    df = orig_df.copy()\n",
    "    \n",
    "    df = clean_1to24(df)\n",
    "    \n",
    "    df.building_year = df.building_year.fillna(df.building_year.median())\n",
    "    df['years_old'] = 2023 - df.building_year\n",
    "    \n",
    "    df = process_lowe_floor_thermal_conductivity(df)\n",
    "    df = process_lower_floor_adjacency_type(df)\n",
    "    df = process_lower_floor_insulation_type(df)\n",
    "    df = process_lower_floor_material(df)\n",
    "    df = process_cat_pct(df, 'main_heat_generators')\n",
    "    df = process_cat_pct(df, 'main_water_heaters')\n",
    "    \n",
    "    df.drop(columns='main_heating_type', inplace=True)\n",
    "    df.drop(columns='main_water_heating_type', inplace=True)\n",
    "    df.drop(columns='nb_commercial_units', inplace=True)\n",
    "    df['nb_dwellings'] = df['nb_dwellings'].clip(upper=50)\n",
    "    df.drop(columns='nb_gas_meters_commercial', inplace=True)\n",
    "    df.drop(columns='nb_gas_meters_housing', inplace=True)\n",
    "    df.drop(columns='nb_gas_meters_total', inplace=True)\n",
    "    df.drop(columns='nb_housing_units', inplace=True)\n",
    "    df.drop(columns='nb_meters', inplace=True)\n",
    "    df.drop(columns='nb_parking_spaces', inplace=True)\n",
    "    df.drop(columns='nb_power_meters_commercial', inplace=True)\n",
    "\n",
    "\n",
    "    df['percentage_glazed_surfaced'] = df['percentage_glazed_surfaced'].fillna(df['percentage_glazed_surfaced'].mean())\n",
    "    df.radon_risk_level = df.radon_risk_level.fillna(df.radon_risk_level.mode().iloc[0])\n",
    "    df['radon_risk_level'] = df['radon_risk_level'].replace({'low': 0, 'medium': 1, 'high': 2})\n",
    "\n",
    "    # From nb_power_meters_housing\n",
    "    df.drop(columns=['building_period'], inplace=True)\n",
    "    df.drop(columns=['building_use_type_description'], inplace=True)\n",
    "    df.drop(columns=['nb_power_meters_housing'], inplace=True)\n",
    "    df.drop(columns=['nb_power_meters_total'], inplace=True)\n",
    "    df.drop(columns=['nb_units_total'], inplace=True)\n",
    "\n",
    "    df = create_one_hot(df, 'outer_wall_materials')\n",
    "    df.outer_wall_thermal_conductivity = df.outer_wall_thermal_conductivity.fillna(df.outer_wall_thermal_conductivity.median())\n",
    "\n",
    "\n",
    "    df.outer_wall_thickness = df.outer_wall_thickness.fillna(df.outer_wall_thickness.mode().iloc[0])\n",
    "    df.outer_wall_thickness = df.outer_wall_thickness.apply(lambda x: float(re.sub(' et -', '', x)))\n",
    "\n",
    "    df.clay_risk_level = df.clay_risk_level.fillna(df.clay_risk_level.mode().iloc[0])\n",
    "    df['clay_risk_level'] = df.clay_risk_level.replace({'low': 0, 'medium': 1, 'high': 2})\n",
    "\n",
    "\n",
    "    df.drop(columns=['consumption_measurement_date'], inplace=True)\n",
    "    df.has_balcony = df.has_balcony.fillna(df.has_balcony.mode().iloc[0])\n",
    "    df = create_one_hot_ColumnOfLists(df, 'heat_generators')\n",
    "    df = create_one_hot_ColumnSplitBySymbol(df, 'heating_energy_source', symbol= ' + ')\n",
    "    df = create_one_hot(df, 'heating_type')\n",
    "    df = create_one_hot(df, 'is_crossing_building')\n",
    "    \n",
    "    df.renewable_energy_sources = df.renewable_energy_sources.fillna('')\n",
    "    df['solar thermal (ecs)'] = df.renewable_energy_sources.apply(lambda x: 'ecs' in x)\n",
    "    df['solar photovoltaic'] = df.renewable_energy_sources.apply(lambda x: 'solar photovoltaic' in x)\n",
    "    df['solar thermal (heating)'] = df.renewable_energy_sources.apply(lambda x: 'heating' in x)\n",
    "    df['solar thermal (hot water)'] = df.renewable_energy_sources.apply(lambda x: 'hot water' in x)\n",
    "    df['solar thermal (DHW)'] = df.renewable_energy_sources.apply(lambda x: 'DHW' in x)\n",
    "    df.drop(columns=['renewable_energy_sources'], inplace=True)\n",
    "    \n",
    "    df = create_one_hot(df, 'roof_material')\n",
    "    \n",
    "    df['thermal_inertia'] = df.thermal_inertia.replace({'low': 0, 'medium': 1, 'high': 2, 'very high': 3})\n",
    "    \n",
    "    df = create_one_hot(df, 'upper_floor_adjacency_type')\n",
    "    \n",
    "    df.upper_floor_insulation_type = df.upper_floor_insulation_type.fillna('INTERNAL')\n",
    "    df['upper_floor_insulation_typeINTERNAL'] = df.upper_floor_insulation_type.apply(lambda x: 'INTERNAL' in x)\n",
    "    df['upper_floor_insulation_typeUNINSULATED'] = df.upper_floor_insulation_type.apply(lambda x: 'UNINSULATED' in x)\n",
    "    df['upper_floor_insulation_typeEXTERNAL'] = df.upper_floor_insulation_type.apply(lambda x: 'EXTERNAL' in x)\n",
    "    df['upper_floor_insulation_typeREFLEXION'] = df.upper_floor_insulation_type.apply(lambda x: 'REFLEXION' in x)\n",
    "    df.drop(columns=['upper_floor_insulation_type'], inplace=True)\n",
    "\n",
    "    df = create_one_hot(df, 'upper_floor_material')\n",
    "    \n",
    "    df.upper_floor_thermal_conductivity = df.upper_floor_thermal_conductivity.fillna(df.upper_floor_thermal_conductivity.median())\n",
    "\n",
    "    df = create_one_hot(df, 'ventilation_type')\n",
    "    \n",
    "    df.wall_insulation_type = df.wall_insulation_type.fillna('internal')\n",
    "    df['wall_insulation_type_internal'] = df.wall_insulation_type.apply(lambda x: 'internal' in x)\n",
    "    df['wall_insulation_type non insulated'] = df.wall_insulation_type.apply(lambda x: 'non insulated' in x)\n",
    "    df['wall_insulation_type external'] = df.wall_insulation_type.apply(lambda x: 'external' in x)\n",
    "    df['wall_insulation_type reflection'] = df.wall_insulation_type.apply(lambda x: 'reflection' in x or 'reflexion' in x)\n",
    "    df['wall_insulation_type insulated'] = df.wall_insulation_type.apply(lambda x: 'insulated' in x)\n",
    "    df.drop(columns=['wall_insulation_type'], inplace=True)\n",
    "    \n",
    "    \n",
    "    df = create_one_hot_ColumnOfLists(df, 'water_heaters')\n",
    "    df = create_one_hot_ColumnSplitBySymbol(df, 'water_heating_energy_source', symbol= ' + ')\n",
    "    df = create_one_hot(df, 'water_heating_type')\n",
    "    df = create_one_hot(df, 'window_filling_type')\n",
    "    df = create_one_hot(df, 'window_frame_material')\n",
    "    df = create_one_hot(df, 'window_glazing_type')\n",
    "    df.window_heat_retention_factor = df.window_heat_retention_factor.fillna(df.window_heat_retention_factor.median())\n",
    "    \n",
    "    df = create_one_hot_ColumnOfLists(df, 'window_orientation')\n",
    "    df.window_thermal_conductivity = df.window_thermal_conductivity.fillna(df.window_thermal_conductivity.median())\n",
    "    \n",
    "    return df\n",
    "df = clean_features(df)\n",
    "test_df = clean_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785d52c0",
   "metadata": {
    "papermill": {
     "duration": 0.522263,
     "end_time": "2023-01-14T21:52:11.515489",
     "exception": false,
     "start_time": "2023-01-14T21:52:10.993226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.loc[:,~df.columns.duplicated()].copy()\n",
    "test_df = test_df.loc[:,~test_df.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d4c5f",
   "metadata": {
    "papermill": {
     "duration": 0.08714,
     "end_time": "2023-01-14T21:52:11.610110",
     "exception": false,
     "start_time": "2023-01-14T21:52:11.522970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = test_df.reindex(columns =df.columns, fill_value=0)\n",
    "df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd8f30a",
   "metadata": {
    "papermill": {
     "duration": 0.005785,
     "end_time": "2023-01-14T21:52:11.622096",
     "exception": false,
     "start_time": "2023-01-14T21:52:11.616311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf60949d",
   "metadata": {
    "papermill": {
     "duration": 0.022286,
     "end_time": "2023-01-14T21:52:11.650474",
     "exception": false,
     "start_time": "2023-01-14T21:52:11.628188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineer(df):\n",
    "    df['volumn'] = df.building_height_ft * df.building_total_area_sqft\n",
    "    return df\n",
    "df = feature_engineer(df)\n",
    "test_df = feature_engineer(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89319af6",
   "metadata": {
    "papermill": {
     "duration": 0.006443,
     "end_time": "2023-01-14T21:52:11.663273",
     "exception": false,
     "start_time": "2023-01-14T21:52:11.656830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48c3759",
   "metadata": {
    "papermill": {
     "duration": 2.501922,
     "end_time": "2023-01-14T21:52:14.171866",
     "exception": false,
     "start_time": "2023-01-14T21:52:11.669944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import imblearn\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import gc\n",
    "import tqdm\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, Ridge, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestRegressor\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool, CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold, RepeatedStratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.ensemble import EasyEnsembleClassifier,BalancedRandomForestClassifier\n",
    "from sklearn import (\n",
    "    decomposition,\n",
    "    discriminant_analysis,\n",
    "    ensemble,\n",
    "    linear_model,\n",
    "    metrics,\n",
    "    model_selection,\n",
    "    naive_bayes,\n",
    "    pipeline,\n",
    "    preprocessing,\n",
    "    svm,\n",
    ")\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import sys\n",
    "\n",
    "def str_to_class(classname):\n",
    "    return getattr(sys.modules[__name__], classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d29ea",
   "metadata": {
    "papermill": {
     "duration": 0.015738,
     "end_time": "2023-01-14T21:52:14.194561",
     "exception": false,
     "start_time": "2023-01-14T21:52:14.178823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_folds(df, target, n_s=5, n_grp=10000):\n",
    "    skf = StratifiedKFold(n_splits=n_s)\n",
    "    grp_target = pd.cut(target, n_grp, labels=False)\n",
    "    return skf.split(grp_target, grp_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f9dcd4",
   "metadata": {
    "papermill": {
     "duration": 0.669217,
     "end_time": "2023-01-14T21:52:14.870378",
     "exception": false,
     "start_time": "2023-01-14T21:52:14.201161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('/kaggle/input/hi-paris-2023/train/train_labels_sent.csv').energy_consumption_per_annum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f054e6f",
   "metadata": {
    "papermill": {
     "duration": 0.015958,
     "end_time": "2023-01-14T21:52:14.892931",
     "exception": false,
     "start_time": "2023-01-14T21:52:14.876973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_NAME = 'LGBMRegressor'\n",
    "# NFOLDS = 3\n",
    "# NESTIMATORS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd7b0a",
   "metadata": {
    "papermill": {
     "duration": 0.016023,
     "end_time": "2023-01-14T21:52:14.915513",
     "exception": false,
     "start_time": "2023-01-14T21:52:14.899490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_train = df[(train_labels< 1200) & (train_labels >=0)]\n",
    "# ys = train_labels[(train_labels< 1200) & (train_labels >=0)]\n",
    "# splits = list(create_folds(X_train, ys, n_s=NFOLDS, n_grp=1000))\n",
    "# X_test = test_df\n",
    "\n",
    "# print(f\"Training {MODEL_NAME}\")\n",
    "# y_preds = np.zeros((NFOLDS, X_test.shape[0]))\n",
    "# y_oof = np.zeros(X_train.shape[0])\n",
    "\n",
    "# for fold_n in range(NFOLDS):\n",
    "#     train_index, valid_index = splits[fold_n]\n",
    "#     print(f\"Fold {fold_n}\")\n",
    "#     X_tr, X_val = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "#     y_tr, y_val = ys.iloc[train_index], ys.iloc[valid_index]  \n",
    "\n",
    "#     dtrain = lgb.Dataset(X_tr, y_tr, free_raw_data=False)\n",
    "#     lgb_params = {\n",
    "#             'n_jobs': -1,\n",
    "#             'verbosity': -1,\n",
    "#             'n_estimators': NESTIMATORS,\n",
    "#         }\n",
    "#     clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200)\n",
    "\n",
    "#     y_oof[valid_index] = clf.predict(X_val) \n",
    "#     y_preds[fold_n, :] = clf.predict(X_test)\n",
    "\n",
    "#     del X_tr, X_val, y_tr, y_val\n",
    "#     gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da6c145",
   "metadata": {
    "papermill": {
     "duration": 0.015007,
     "end_time": "2023-01-14T21:52:14.937277",
     "exception": false,
     "start_time": "2023-01-14T21:52:14.922270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('Cross-validation score:')\n",
    "# explained_variance_score(ys, y_oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ed3827",
   "metadata": {
    "papermill": {
     "duration": 0.01541,
     "end_time": "2023-01-14T21:52:14.959245",
     "exception": false,
     "start_time": "2023-01-14T21:52:14.943835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sub = pd.read_csv('/kaggle/input/hi-paris-2023/sample_submission_sent.csv')\n",
    "# sub.energy_consumption_per_annum = np.mean(np.mean(all_y_preds, axis=0),axis=0)\n",
    "# sub.to_csv('submission_6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8656316",
   "metadata": {
    "papermill": {
     "duration": 0.016308,
     "end_time": "2023-01-14T21:52:14.981948",
     "exception": false,
     "start_time": "2023-01-14T21:52:14.965640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sub.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede3652f",
   "metadata": {
    "papermill": {
     "duration": 0.005983,
     "end_time": "2023-01-14T21:52:14.994609",
     "exception": false,
     "start_time": "2023-01-14T21:52:14.988626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacd3064",
   "metadata": {
    "papermill": {
     "duration": 0.016546,
     "end_time": "2023-01-14T21:52:15.017374",
     "exception": false,
     "start_time": "2023-01-14T21:52:15.000828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAMES = ['XGBRegressor', 'LGBMRegressor', 'CatBoostRegressor']\n",
    "STACKING_MODEL = 'Ridge'\n",
    "NFOLDS = 5\n",
    "NESTIMATORS = 100\n",
    "NESTIMATORS_STACK = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cab353",
   "metadata": {
    "papermill": {
     "duration": 1569.935726,
     "end_time": "2023-01-14T22:18:24.960500",
     "exception": false,
     "start_time": "2023-01-14T21:52:15.024774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = df[(train_labels< 1200) & (train_labels >=0)]\n",
    "ys = train_labels[(train_labels< 1200) & (train_labels >=0)]\n",
    "splits = list(create_folds(X_train, ys, n_s=NFOLDS, n_grp=1000))\n",
    "X_test = test_df\n",
    "\n",
    "all_y_preds = []\n",
    "all_y_oof = []\n",
    "train_scores = []\n",
    "\n",
    "for MODEL_NAME in MODEL_NAMES:\n",
    "    print(f\"Training {MODEL_NAME}\")\n",
    "\n",
    "    y_preds = np.zeros((NFOLDS, X_test.shape[0]))\n",
    "    y_oof = np.zeros(X_train.shape[0])\n",
    "\n",
    "    for fold_n in range(NFOLDS):\n",
    "        train_index, valid_index = splits[fold_n]\n",
    "        print(f\"Fold {fold_n}\")\n",
    "        X_tr, X_val = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "        y_tr, y_val = ys.iloc[train_index], ys.iloc[valid_index]  \n",
    "        \n",
    "        if MODEL_NAME=='XGBRegressor':\n",
    "            clf = str_to_class(MODEL_NAME)(tree_method=\"hist\", enable_categorical=True, n_estimators=NESTIMATORS)\n",
    "            clf.fit(X_tr, y_tr)\n",
    "        elif MODEL_NAME=='LGBMRegressor': \n",
    "            \n",
    "            dtrain = lgb.Dataset(X_tr, y_tr, free_raw_data=False)\n",
    "            lgb_params = {\n",
    "                    'n_jobs': -1,\n",
    "                    'verbosity': -1,\n",
    "                    'n_estimators': NESTIMATORS,\n",
    "                }\n",
    "            clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200)\n",
    "        elif MODEL_NAME=='CatBoostRegressor':\n",
    "            clf = str_to_class(MODEL_NAME)(n_estimators=NESTIMATORS)\n",
    "            clf.fit(X_tr.to_numpy(), y_tr.to_numpy(), verbose=0)\n",
    "            \n",
    "        y_preds_tr = clf.predict(X_tr)\n",
    "        y_preds_val = clf.predict(X_val)\n",
    "        y_oof[valid_index] = y_preds_val\n",
    "        y_preds[fold_n, :] = clf.predict(X_test)\n",
    "        val_score = explained_variance_score(y_val, y_preds_val)\n",
    "        train_score = explained_variance_score(y_tr, y_preds_tr)\n",
    "        print(f\"Val score: {val_score:.3g}\")\n",
    "        print(f\"Train score: {train_score:.3g}\")\n",
    "\n",
    "        train_scores.append(train_score)\n",
    "\n",
    "        del X_tr, X_val, y_tr, y_val\n",
    "        gc.collect() \n",
    "            \n",
    "    all_y_preds.append(y_preds)\n",
    "    all_y_oof.append(y_oof)\n",
    "# all_y_oof = np.concatenate(all_y_oof, axis=-1)\n",
    "# all_y_preds = np.concatenate(all_y_preds, axis=-1)\n",
    "# all_y_oof = np.array(all_y_oof).reshape(-1, len(MODEL_NAMES))\n",
    "# all_y_preds = np.array(all_y_preds).reshape(NFOLDS, -1, len(MODEL_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e615a21",
   "metadata": {
    "papermill": {
     "duration": 0.028658,
     "end_time": "2023-01-14T22:18:24.995964",
     "exception": false,
     "start_time": "2023-01-14T22:18:24.967306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Average of 3 models\n",
    "score = explained_variance_score(ys, np.mean(all_y_oof, axis=0))  # scoring\n",
    "print(f\"Overall OOF score: {score:.3g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a165962",
   "metadata": {
    "papermill": {
     "duration": 0.996866,
     "end_time": "2023-01-14T22:18:26.000551",
     "exception": false,
     "start_time": "2023-01-14T22:18:25.003685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/kaggle/input/hi-paris-2023/sample_submission_sent.csv')\n",
    "sub.energy_consumption_per_annum = np.mean(np.mean(all_y_preds, axis=0),axis=0)\n",
    "sub.to_csv('submission_6.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6bce90",
   "metadata": {
    "papermill": {
     "duration": 0.016232,
     "end_time": "2023-01-14T22:18:26.024672",
     "exception": false,
     "start_time": "2023-01-14T22:18:26.008440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Ridge stacking\n",
    "# y_stk_val = np.zeros(X_train.shape[0])\n",
    "# y_stk_test = np.zeros(X_test.shape[0])\n",
    "# for nestimators in tqdm.tqdm(range(NESTIMATORS_STACK)):\n",
    "#     stk = str_to_class(STACKING_MODEL)(alpha=0.1)\n",
    "#     stk.fit(all_y_oof, ys.values)\n",
    "#     y_stk_val += stk.predict(all_y_oof) / NESTIMATORS_STACK\n",
    "#     for all_y_pred in all_y_preds:\n",
    "#         y_stk_test += stk.predict(all_y_pred) / (NESTIMATORS_STACK * NFOLDS)\n",
    "# score = explained_variance_score(ys, y_stk_val)  # scoring\n",
    "# print(f\"Overall OOF score: {score:.3g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c92b6",
   "metadata": {
    "papermill": {
     "duration": 0.01583,
     "end_time": "2023-01-14T22:18:26.048636",
     "exception": false,
     "start_time": "2023-01-14T22:18:26.032806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# score = explained_variance_score(ys, y_stk_val)  # scoring\n",
    "# print(f\"Overall OOF score: {score:.3g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069dcbd4",
   "metadata": {
    "papermill": {
     "duration": 0.007202,
     "end_time": "2023-01-14T22:18:26.063692",
     "exception": false,
     "start_time": "2023-01-14T22:18:26.056490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1749.844874,
   "end_time": "2023-01-14T22:18:27.097449",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-14T21:49:17.252575",
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
